---
title: AI Tech 4주차 회고
categories: ['Retrospect']
tags: []
image: /assets/img/previews/resized/ai_tech.png
math: true
---

## 1일차 - 🎩

### 새롭게 알게 된 내용
---

#### 1. BPR Loss

BRP loss는 기존의 pointwise loss, 즉 각각의 데이터마다 loss를 계산하는 방식이 아닌 pairwise loss, 데이터 쌍에서 loss를 계산한다.

BRP loss는 데이터셋에서 positive sample과 negative sample을 추출하고 positive sample이 negative sample보다 더 높은 점수로 평가되도록 유도한다. 이의 식은 아래와 같다.

$$
BRP loss = -\sum_{(u, i, j) \in D}ln \sigma(\hat{x}_{ui} - \hat{y}_{uj})
$$

여기서 $$\hat{x}_{ui}$$ 는 positive sample의 점수고 $\hat{y}_{uj}$ 는 negative sample의 점수다.

sampling의 방식에 따라 모델의 성능이 달라질 수 있으니, sampling 전략 또한 잘 세워야한다.

### 회고
---
Factorization Machine에 대한 논문 리뷰를 진행하였다. 단순한 collaborative filtering에서 나아가 여러가지 feature까지 추가로 다룰 수 있다는 점이 인상깊었다. 데이터에 interaction을 제외한 feature가 많다면 한 번 사용해봐야겠다.


## 2일차 - 🤯

### 새롭게 알게 된 내용
---

#### 1. reparameterization trick

parameter $\mu$와 $\sigma$가 존재하고 각각 latent factor space의 평균과 표준편차라 하자. 
이의 분포를 직접적으로 구현해 latent factor를 sampling하게 되면 gradient가 $\mu$, $\sigma$ 이전으로 흐르지 못하게 된다.
따라서 아래와 같이 latent factor를 구한다.

$$
z = \mu + \sigma\cdot\epsilon
$$

이때 $\epsilon$은 정규분포 $N(0, 1)$에서 sampling된다.
이런식으로 하면 gradient가 $\mu$, $\sigma$ 이전으로 잘 흐르게 되어 이전 파라미터의 학습을 가능케 한다.

#### 2. Jensen's Inequality

$\varphi$가 convex할 경우, $\varphi(E[X]) \le E[\varphi(X)]$가 성립한다.

반대로 $\varphi$가 concave할 경우, $\varphi(E[X]) \ge E[\varphi(X)]$가 성립한다.

이러한 inequality는 각종 증명에 유용하게 사용된다.

### 회고
---
강의가 4개라 이번 주는 편할 줄 알았는데 대학원 진학을 다시 생각해볼 정도로 난이도가 너무 높았다...
일단 이해안되는 부분이 있어도 조금씩 넘어가며 듣자.
egoing님이 진행하신 Github 특강은 어느정도 아는 내용이라 솔직히 좀 지루했다. 내일은 조금 더 심화된 내용을 다루니 기대해봐야겠다.

## 3일차 - 🏷️

### 새롭게 알게 된 내용
---

#### 1. git에 대한 다양한 지식들

* branch는 기본적으로 commit을 가리키는 pointer이다.
* git checkout을 통해 HEAD가 가리키는 branch를 변경할 수 있다.
* git checkout을 통해 HEAD가 직접적으로 commit을 가리키게 할 수 있다. (detached HEAD)
* git reset을 통해 branch가 가리키는 commit을 변경할 수 있다.
* git reflog로 git reset등으로 접근할 수 없게된 commit을 찾을 수 있다.

<https://learngitbranching.js.org/?locale=ko>

### 회고
---
잠을 별로 못자서 그런지 GitHub 특강에 제대로 집중하지 못했다.
부캠을 시작한지 얼마안됐는데 벌써부터 해이져서는 안된다.
밤 1시 이후부터는 절대로 핸드폰을 쓰지 않기로 다짐하자.

## 4일차 - ✨

### 새롭게 알게 된 내용
---

#### 1. Factorization Machines

<https://joshua5301.github.io/posts/factorization-machines/>

### 회고
---
오늘은 LightGCN 논문 구현과 더불어 월요일날 리뷰했던 FM 모델에 대한 블로그 글을 작성하였다.
1, 2주차와 다르게 하루하루 부지런히 지내고 있는 것 같아 기쁘다.
다만 논문을 구현하는 과정이 생각보다 어렵고 막막한 과정이라는 깨달았다. 막히는 부분이 있다면 적극적으로 조원분들이나 멘토님께 여쭤보자.


## 5일차 - 🤖

### 새롭게 알게 된 내용
---

#### 1. 기본행연산과 기본행렬

행렬 $A$에 대하여 행렬의 행에 대한 다음 세 연산은 기본행연산이라 한다.

1. $A$의 두 행을 교환하는 것
2. $A$의 한 행에 영이 아닌 스칼라를 곱하는 것
3. $A$의 한 행에 다른 행의 스칼라 배를 더하는 것

또한 이러한 세 연산을 항등행렬 $I_n$에 적용하여 얻은 행렬을 기본행렬이라 한다.

#### 2. 행렬의 랭크과 역행렬

행렬 $A$의 랭크는 선형변환 $L_A: F^n \rightarrow F^m$ 의 랭크로 정의하고 $rank(A)$라 표기한다.
또한 $n \times n$ 행렬이 가역이기 위한 필요충분조건은 행렬의 랭크가 $n$인 것이다.

### 회고
---
강의 내용을 전부 이해하지 못한 채로 한 주가 끝나 찝찝한 기분이다. 다음주 월요일날 VAE 논문 리뷰를 하기로 했으니, 그 전에 이의 근간이 되는 VI에 대해 완벽히 이해하고 싶다. 주말동안 미처 못끝냈던 논문 구현과 더불어 조원분이 보내주신 강의를 들어 VI와 EM에 대해 완벽히 이해하도록 하자.