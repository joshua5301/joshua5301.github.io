---
title: AI Tech 1주차 회고
categories: ['Retrospect']
tags: ['Pytorch', 'Python']
image: /assets/img/previews/resized/ai_tech.png
math: true
---

## 1일차 - 🎉

### 새롭게 알게 된 내용
---

#### 1. Tensor는 overflow에 취약하다.

생성시에는 오버플로우를 감지해주지만, type casting 때나 연산을 할 때는 감지해주지 않는 듯하다.
Pytorch를 사용하는 도중 오버플로우가 일어나는 상황은 거의 없을 것 같지만 일단은 알아두자.
~~~python
# torch.tensor(99999999, dtype=torch.int8)
# 오버플로우로 인한 에러

x = torch.tensor(99999999)
x = x.to(torch.int8)
print(x)
# 오버플로우가 발생하지만 에러는 일어나지 않는다.

x = torch.tensor(99, dtype=torch.int8)
y = torch.tensor(99, dtype=torch.int8)
z = x + y
print(z)
# 오버플로우가 발생하지만 에러는 일어나지 않는다.
~~~

### 회고
---

첫날이라 긴장해서 그런지 많이 피곤했고, 어느정도 아는 내용이라 공부에 집중이 잘 안되었다.
시작이 반이라지만 이렇게 하면 절반에서 멈출 듯하다.
아는 내용은 배속을 걸거나 조금씩 스킵해가며 듣자. 내일부터는 본격적으로 시작해볼 것이다.

점수: 3/10

<br />

## 2일차 - 🌻

### 새롭게 알게 된 내용
---

#### 1. contiguous하지 않은 텐서라도 view가 가능할 때가 있다.

언제 view와 복사없는 reshape가 가능해지는지 pytorch의 reshape documentation에 자세히 나와있으니 참고로 알아두자.
다만 documentation에서도 나와있듯이 이러한 동작에 의존하는 코드를 짜서는 안된다.

~~~python
x = torch.ones(2, 3, 4)

print(x.stride())
print(x.is_contiguous())
x_slice = x[:, :, -1]

# contiguous하지 않지만 view가 가능하다.
print(x_slice.stride())
print(x_slice.is_contiguous())
x_view = x_slice.view(3, 2)
~~~

#### 2. matmul vs mm vs bmm

matmul: broadcasting 가능, 호환되는 모든 차원의 tensor 연산 가능 <br />
mm: broadcasting 불가, 오직 2차원 tensor만 <br />
bmm: broadcasting 불가, 오직 3차원 tensor만

#### 3. 딕셔너리의 키가 없을 때의 접근

일반적인 상황의 경우 dict.get이나 collections.defaultdict을 사용하자. <br />
복잡한 상황일 때는 dict를 상속하고 __missing__을 작성하자.

<https://joshua5301.github.io/posts/missing-key/>

### 회고
---

아침에 블로그 글도 하나 쓰고, 강의를 들으면서 이것저것 생각해보는 등 나름 알찬 하루를 보낸 것 같다.
다만 오후 시간에 좀 빈둥거리며 지냈던 것이 아쉽다. 운동이나 악기 연주 등 건전한 취미를 하나 만들어 공부에 지쳤을 때 해보는 것도 좋을 것 같다.

점수: 7/10

<br />

## 3일차 - 🍃

### 새롭게 알게 된 내용
---

#### 1. torch.method vs torch.Tensor.method

텐서를 다루는 몇몇 연산들은 torch.method의 형태로도 쓸 수 있고 torch.Tensor.method로도 쓸 수 있다.

~~~python
x, y = torch.arange(4), torch.arange(4)
# 함수형 접근
z2 = torch.dot(x, y)
# 객체지향적 접근
z1 = x.dot(y)
print(z1, z2)
~~~

함수형 접근과 객체지향적 접근을 둘 다 제공해주어 취향에 따라 사용하라고 제공해주고 있는 것 같다.
개인적으로 dot이나 matmul 같은 동등한 두 텐서를 대상으로 하는 연산은 함수형 접근(torch.method)을,
하나의 텐서를 대상으로 하거나 연산의 주가 되는 텐서가 존재할 때는 객체지향적 접근(torch.Tensor.method)을 사용하는 것이 나아보인다.

#### 2. Pytorch의 다양한 cross-entropy loss

BCELoss: 2개의 클래스만 존재할 때, 확률을 input으로 받음

BCEWithLogitsLoss: 2개의 클래스만 존재할 때, logit을 input으로 받음 <br /> 
(즉, softmax함수를 통과시켜서는 안된다.)

CrossEntropyLoss: 여러개의 클래스가 존재할 때, logit을 input으로 받음 <br />
(즉, softmax함수를 통과시켜서는 안된다.)

### 회고
---
오늘은 컨디션이 왠지 좋지 않아 강의에 집중이 되지 않았다. 팀원들과 강의를 어디까지 들을지 정해놓았기 때문에 억지로
들었지만, 지식이 머릿속에 들어오지 않는 기분이다. 강의 내용이 어느정도 아는 내용이라 그나마 다행이다.
평소에 운동을 하는 등 컨디션 관리 역시 신경써야겠다.

점수: 3/10

<br />

## 4일차 - 📝

### 새롭게 알게 된 내용
---

#### 1. SGD와 mini-batch는 모두 비복원추출

지금까지 SGD와 mini-batch gradient descent는 모두 복원추출로 랜덤하게 이루어지는 줄 알았다.
하지만 이는 비복원추출로 이루어지며, 따라서 1000개의 데이터를 1 epoch만큼 돌리면 정확히 1000번의 가중치 업데이트가 이루어진다.

#### 2. SVM

SVM은 기존의 로지스틱 회귀와 마찬가지로 입력 벡터 공간에서 선형 결정 경계를 찾는다.
다만 결정 경계와 가장 가까이 있는 양쪽 데이터 포인트(서포트 벡터)의 거리를 최대화시킨다는 점이 다르다.
이는 결정 경계의 일반화 성능을 높여준다.
나아가 SVM은 kernel trick을 통해 입력 벡터 공간의 차원을 임의로 확장함으로써 비선형분류를 가능케 한다.

### 회고
---
오늘은 밀려있던 모든 과제를 끝마쳤다. 강의로만 보던 코드 조각들을 직접 구현함으로써 모호했던 지식을 비로소 내 것으로 만든 기분이 든다.
오늘의 성과와 별개로, SVM과 He initialization에 대해 알아보던 중 내가 수학 지식이 부족하다는 생각이 들었다.
앞으로 다양한 논문을 읽게 될텐데 벌써부터 발목이 잡히면 안되니 하루빨리 수학 공부를 시작해야겠다.
일단 전에 하던 선형대수학 공부부터 다시 하고 시간이 되면 통계학도 공부해야지.

점수: 7/10

<br />

## 5일차 - 🔥

### 새롭게 알게 된 내용
---

#### 1. 왈러스 연산자

왈러스 연산자를 코드에 적극 도입하도록 하자.

<https://joshua5301.github.io/posts/walrus-operator/>

#### 2. 부분공간

프리드버그 선형대수학 1.3 절에 대해 간단히 정리하였다.

<https://joshua5301.github.io/posts/friedberg-01-03/>

### 회고
---

블로그를 짧지만 하루에 2편이나 썼다. 별 것 아닌 것처럼 보이지만 글쓰기를 정말 싫어하는 나로서는 큰 발전이라 생각한다.
나아가 오랜만에 잠시 놓았던 수학을 공부하니 재밌기도 하고 성취감이 느껴진다.
오늘 공부량이 그렇게 많지는 않았지만 이런식으로 꾸준히 페이스를 잃지 않고 진행하는 것도 좋을 듯 싶다.

점수: 8/10